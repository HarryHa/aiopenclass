{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-based Recommender "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've provided a sample dataset of outdoor clothing and products from Patagonia. The data looks like this, and you can view the whole thing (~550kb) on Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| id | description                                                                 |\n",
    "|----|-----------------------------------------------------------------------------|\n",
    "|  1 | Active classic boxers - There's a reason why our boxers are a cult favori...|\n",
    "|  2 | Active sport boxer briefs - Skinning up Glory requires enough movement wi...|\n",
    "|  3 | Active sport briefs - These superbreathable no-fly briefs are the minimal...|\n",
    "|  4 | Alpine guide pants - Skin in, climb ice, switch to rock, traverse a knife...|\n",
    "|  5 | Alpine wind jkt - On high ridges, steep ice and anything alpine, this jac...|\n",
    "|  6 | Ascensionist jkt - Our most technical soft shell for full-on mountain pur...|\n",
    "|  7 | Atom - A multitasker's cloud nine, the Atom plays the part of courier bag...|\n",
    "|  8 | Print banded betina btm - Our fullest coverage bottoms, the Betina fits h...|\n",
    "|  9 | Baby micro d-luxe cardigan - Micro D-Luxe is a heavenly soft fabric with ...|\n",
    "| 10 | Baby sun bucket hat - This hat goes on when the sun rises above the horiz...|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it; just IDs and text about the product in the form Title - Description. We're going to use a simple Natural Language Processing technique called TF-IDF (Term Frequency - Inverse Document Frequency) to parse through the descriptions, identify distinct phrases in each item's description, and then find 'similar' products based on those phrases.\n",
    "TF-IDF works by looking at all (in our case) one, two, and three-word phrases (uni-, bi-, and tri-grams to NLP folks) that appear multiple times in a description (the \"term frequency\") and divides them by the number of times those same phrases appear in all product descriptions. So terms that are 'more distinct' to a particular product (\"Micro D-luxe\" in item 9, above) get a higher score, and terms that appear often, but also appear often in other products (\"soft fabric\", also in item 9) get a lower score.\n",
    "Once we have the TF-IDF terms and scores for each product, we'll use a measurement called cosine similarity to identify which products are 'closest' to each other.\n",
    "Luckily, like most algorithms, we don't have to reinvent the wheel; there are ready-made libraries that will do the heavy lifting for us. In this case, Python's SciKit Learn has both a TF-IDF and cosine similarity implementation. I've put the whole thing together in a Flask app that will actually serve recommendations over a REST API, as you might do in production (in fact, the code is not very different from what we actually do run in production at Grove).\n",
    "The engine has a .train() method that runs TF-IDF across the input products file, computes similar items for every item in the set, and stores those items along with their cosine similarity, in Redis. The .predict method just takes an item ID and returns the precomputed similarities from Redis. Dead simple!\n",
    "The engine code in its entirety is below. The comments explain how the code works, and you can explore the complete Flask app on Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pic/tfidf.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import redis\n",
    "from flask import current_app\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "\n",
    "def info(msg):\n",
    "    current_app.logger.info(msg)\n",
    "\n",
    "\n",
    "class ContentEngine(object):\n",
    "\n",
    "    SIMKEY = 'p:smlr:%s'\n",
    "\n",
    "    def __init__(self):\n",
    "        self._r = redis.StrictRedis.from_url(current_app.config['REDIS_URL'])\n",
    "\n",
    "    def train(self, data_source):\n",
    "        start = time.time()\n",
    "        ds = pd.read_csv(data_source)\n",
    "        info(\"Training data ingested in %s seconds.\" % (time.time() - start))\n",
    "\n",
    "        # Flush the stale training data from redis\n",
    "        self._r.flushdb()\n",
    "\n",
    "        start = time.time()\n",
    "        self._train(ds)\n",
    "        info(\"Engine trained in %s seconds.\" % (time.time() - start))\n",
    "\n",
    "    def _train(self, ds):\n",
    "        \"\"\"\n",
    "        Train the engine.\n",
    "\n",
    "        Create a TF-IDF matrix of unigrams, bigrams, and trigrams\n",
    "        for each product. The 'stop_words' param tells the TF-IDF\n",
    "        module to ignore common english words like 'the', etc.\n",
    "\n",
    "        Then we compute similarity between all products using\n",
    "        SciKit Leanr's linear_kernel (which in this case is\n",
    "        equivalent to cosine similarity).\n",
    "\n",
    "        Iterate through each item's similar items and store the\n",
    "        100 most-similar. Stops at 100 because well...  how many\n",
    "        similar products do you really need to show?\n",
    "\n",
    "        Similarities and their scores are stored in redis as a\n",
    "        Sorted Set, with one set for each item.\n",
    "\n",
    "        :param ds: A pandas dataset containing two fields: description & id\n",
    "        :return: Nothin!\n",
    "        \"\"\"\n",
    "\n",
    "        tf = TfidfVectorizer(analyzer='word',\n",
    "                             ngram_range=(1, 3),\n",
    "                             min_df=0,\n",
    "                             stop_words='english')\n",
    "        tfidf_matrix = tf.fit_transform(ds['description'])\n",
    "\n",
    "        cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "        for idx, row in ds.iterrows():\n",
    "            similar_indices = cosine_similarities[idx].argsort()[:-100:-1]\n",
    "            similar_items = [(cosine_similarities[idx][i], ds['id'][i])\n",
    "                             for i in similar_indices]\n",
    "\n",
    "            # First item is the item itself, so remove it.\n",
    "            # This 'sum' is turns a list of tuples into a single tuple:\n",
    "            # [(1,2), (3,4)] -> (1,2,3,4)\n",
    "            flattened = sum(similar_items[1:], ())\n",
    "            self._r.zadd(self.SIMKEY % row['id'], *flattened)\n",
    "\n",
    "    def predict(self, item_id, num):\n",
    "        \"\"\"\n",
    "        Couldn't be simpler! Just retrieves the similar items and\n",
    "        their 'score' from redis.\n",
    "\n",
    "        :param item_id: string\n",
    "        :param num: number of similar items to return\n",
    "        :return: A list of lists like: [[\"19\", 0.2203],\n",
    "        [\"494\", 0.1693], ...]. The first item in each sub-list is\n",
    "        the item ID and the second is the similarity score. Sorted\n",
    "        by similarity score, descending.\n",
    "        \"\"\"\n",
    "\n",
    "        return self._r.zrange(self.SIMKEY % item_id,\n",
    "                              0,\n",
    "                              num-1,\n",
    "                              withscores=True,\n",
    "                              desc=True)\n",
    "\n",
    "content_engine = ContentEngine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
